# Escherichia coli AMR and gene network prediction

[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/projects/miniconda/en/latest/)
[![Snakemake](https://img.shields.io/badge/snakemake--green)](https://snakemake.readthedocs.io/en/stable/)

## Purpose

In this project, we aim to perform variant calling on about 8000 *E. coli* strains whose genome sequencing files are available and for whom we also were able to obtain antimicrobial resistance phenotypes.
Briefly, variant calling consists in identifying all the genetic variations and their associated genotype in a population compared to a reference genome. This is performed by aligning sequencing reads for each sample of the population againts a reference genome, then by identifying polymorphic regions in the population, and finally characterizing variants and their genotypes at each of these polymorphic regions.

This repository regroups the different codes, datasets and results that cover these different steps for variant calling:
1- Creation of the E. coli reference pangenome (to be added to repo)  
2- Variant calling 

## Installation and Setup

This repository uses Snakemake to run the pipeline and conda to manage software environments and installations. You can find operating system-specific instructions for installing miniconda [here](https://docs.conda.io/projects/miniconda/en/latest/). After installing conda and [mamba](https://mamba.readthedocs.io/en/latest/), run the following command to create the pipeline run environment.

```{bash}
TODO: Replace <NAME> with the name of your environment
mamba env create -n <NAME> --file envs/dev.yml
conda activate <NAME>
```

Snakemake manages rule-specific environments via the `conda` directive and using environment files in the [envs/](./envs/) directory. Snakemake itself is installed in the main development conda environment as specified in the [dev.yml](./envs/dev.yml) file.

To start the pipeline, run:

```{bash}
snakemake --software-deployment-method conda -j 8
```

**Tips for Developers**

You can use the following command to export your current conda environment to a `yml` file.  
This command will only export the packages that you have installed directly, not the ones that were installed as dependencies. When you're ready to share, please delete this section.

```{bash}
conda env export --from-history --no-builds > envs/dev.yml
```

## Data

TODO: Add details about the description of input / output data and links to Zenodo depositions, if applicable.

***E. coli* reference pangenome**
TBD

**Variant calling workflow - Development & Test**

The test data used to develop this workflow consist in the sequencing files (fastq.gz read1 and read2) of 5 different *E. coli* strains + the pangenome of 72 *E. coli* strains from the ECOR collection. *Note: the pangenome contains both cds and intergenic regions*

SRA accession numbers:
Currently, these can be found in a s3 bucket (which contains a total of 25 samples) 

The test generated bam and bai bam and bam.bai files. Each bam file has been sorted, marked for duplicates, added rg# and indexed. For now, the bams are too large to be stored on Github, and not yet deposited on Zenodo.

## Overview

### Description of the folder structure

**Test dataset and workflow development**
A specific folder 'Test' has been created for developpment and testing of the workflow.
Subfolders:
    - vcf_call: folder containing the vcf file generated y the joined-variant calling and as well as the stats file assocaiyed with the vcf file
    - pangenome: folder containing the fasta file of the pangenome along with the different files generated by `bwa index` of the pangenome 
Files:
    - snakefile of the workflow, file is named: 'workflow_pipped'
    - the list of bam files and their location 'list_bam'

Note: while the snakefile mention fastq and bam_files subfolders  

### Description of how the tool works

To run the workflow you need to:
    - create the fastq folder and add the fastq.gz associayed with the samples of interest
    - create the bam_files folder
    - create the bam_list file with the name and expected location of the bam and bam.bai files generated by the workflow
    - create bechmark folder to store benchmark files generated for each rule
    - edit the snakefile to match the path for each directory
    - in rule aln_and_format: l47 edit the path to the reference pangenome if necessary
                              l48 edit the path to PICARD to match its location in your system
    - in rule vcf_calling: l73 edit the path to the reference pangenome if necessary   

**Tips for Developers**

You should consider having a quickstart guide for users who want to run the pipeline, and/or a demo dataset that they can use to test the pipeline.  
When you're ready to share, please delete this section.

### Compute Specifications

TODO: Describe what compute resources were used to run the analysis. For example, you could list the operating system, number of cores, RAM, and storage space.

## Contributing

See how we recognize [feedback and contributions to our code](https://github.com/Arcadia-Science/arcadia-software-handbook/blob/main/guides-and-standards/guide-credit-for-contributions.md).

---
## For Developers

This section contains information for developers who are working off of this template. Please delete this section when you're ready to share your repository.

### GitHub templates
This template uses GitHub templates to provide checklists when making new pull requests as well as templates for issues, which could be used to request new features or report bugs. These templates are stored in the [.github/](./.github/) directory.

### VSCode
This template includes recommendations to VSCode users for extensions, particularly the `ruff` linter. These recommendations are stored in `.vscode/extensions.json`. When you open the repository in VSCode, you should see a prompt to install the recommended extensions. 

### `.gitignore`
This template uses a `.gitignore` file to prevent certain files from being committed to the repository.

### `pyproject.toml`
`pyproject.toml` is a configuration file to specify your project's metadata and to set the behavior of other tools such as linters, type checkers etc. You can learn more [here](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/)

### Linting
This template automates linting and formatting using GitHub Actions and the `ruff` and `snakefmt` linters. When you push changes to your repository, GitHub will automatically run the linter and report any errors, blocking merges until they are resolved.

### Testing
This template uses GitHub Actions to automate a test dry run of the pipeline. When you push changes to your repository, GitHub will automatically run the tests and report any errors, blocking merges until they are resolved.
